# -*- coding: utf-8 -*-
"""CNN-LSTM_with_XGB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18PweI8mUNb7a7RkNT1cCaLuMo0OAMhGh
"""

# import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from tensorflow import keras
from keras import models, layers

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/hack24/Hackathon_Data.xlsx')


xlsx_file = pd.ExcelFile('/content/drive/MyDrive/Colab Notebooks/hack24/Hackathon_Data.xlsx')
sheet_names = xlsx_file.sheet_names
df = xlsx_file.parse(sheet_names[0])

#cutting last 3 weeks as testing data

cutoff_date = pd.to_datetime('2024-03-09 00:00:00')

training_data = df[df['DateAndHour'] < cutoff_date]
test_data = df[df['DateAndHour'] >= cutoff_date]

test_data.loc[df['DateAndHour'].dt.date == pd.to_datetime('2024-03-30').date(), 'Load_data'] = 0
test_data.loc[df['DateAndHour'].dt.date == pd.to_datetime('2024-03-26').date(), 'Load_data'] = 0

def create_features(df):

  """ A function to create features in a dataset """


  # Extract temporal features from 'Datetime'
  df['Temp2'] = df["Temperature"]**2
  df['Hour'] = df.index.hour
  df['DayOfWeek'] = df.index.dayofweek
  df['Month'] = df.index.month
  df['is_weekend'] = np.where(df['DayOfWeek'] < 5, 0, 1)

  # 1. Lag Features
  df['load_lag_1h'] = df['Load_data'].shift(1)
  df['load_lag_24h'] = df['Load_data'].shift(24)
  df['load_lag_24h'].iloc[:23] = 0
#     df['load_lag_168h'] = df['Load_data'].shift(168)
#     df['load_lag_168h'].iloc[:167] = 0# 1 week

  # 2. Rolling Statistics
  df['load_rolling_mean_24h'] = df['Load_data'].rolling(window=24).mean()


  df['temp_rolling_mean_2'] = df['Temperature'].rolling(window=2).mean()
  df['temp_rolling_mean_2'].iloc[:1] = 0  # Set the first 23 values to 0
  df['temp_rolling_mean_24'] = df['Temperature'].rolling(window=24).mean()
  df['temp_rolling_mean_24'].iloc[:23] = 0  # Set the first 23 values to 0

  # Cyclical features for month, day of week, and hour
  df['day_of_week_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)
  df['day_of_week_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)

  df['hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)
  df['hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)

  return df

###Uncomment below lines for first run only.:
# training_data.set_index('DateAndHour', inplace=True)
# test_data.set_index('DateAndHour', inplace=True)
training_data = create_features(training_data)
test_data = create_features(test_data)

from sklearn.preprocessing import MinMaxScaler, PowerTransformer, StandardScaler, RobustScaler

# Select features for the model
features = ['Load_data', 'Temperature','Temp2', 'Hour', 'DayOfWeek', 'Month','is_weekend','temp_rolling_mean_2','temp_rolling_mean_24','day_of_week_sin','day_of_week_cos','hour_sin','hour_cos']

# # Normalize the features
scaler = MinMaxScaler(feature_range=(0, 1))
# # scaled_data_train = scaler.fit_transform(training_data[features])
# scaler = RobustScaler()
scaled_data_training = scaler.fit_transform(training_data[features])
scaled_data_test = scaler.fit_transform(test_data[features])

# scaled_data_training = training_data[features]
# scaled_data_test = test_data[features]

# Create sequences
def create_sequences(data, sequence_length):
    X = []
    y = []
    for i in range(len(data) - sequence_length):
        X.append(data[i:(i + sequence_length), :])
        y.append(data[i + sequence_length, 0])  # 0 index is 'Load_data'
    return np.array(X), np.array(y)

sequence_length = 24
X, y = create_sequences(scaled_data_training, sequence_length)
X_train, y_train = create_sequences(scaled_data_training, sequence_length)
X_test, y_test = create_sequences(scaled_data_test, sequence_length)



# from sklearn.model_selection import train_test_split

# # # Split into train and test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)

print("Training data shape:", X_train.shape)
print("Testing data shape:", X_test.shape)

import tensorflow as tf
from tensorflow.keras.layers import Layer
from tensorflow.keras import backend as K
from tensorflow import keras

# Make sure to add @keras.utils.register_keras_serializable decorator
@keras.utils.register_keras_serializable()
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W_a = self.add_weight(name='W_a',
                                   shape=(input_shape[2], input_shape[2]),
                                   initializer='uniform',
                                   trainable=True)
        self.U_a = self.add_weight(name='U_a',
                                   shape=(input_shape[1], input_shape[2]),
                                   initializer='uniform',
                                   trainable=True)
        self.V_a = self.add_weight(name='V_a',
                                   shape=(input_shape[2], 1),
                                   initializer='uniform',
                                   trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        # Hidden state representation
        hidden_states = inputs

        # Score function
        score = tf.tensordot(tf.tanh(tf.tensordot(hidden_states, self.W_a, axes=1) + self.U_a), self.V_a, axes=1)

        # Attention weights
        attention_weights = tf.nn.softmax(score, axis=1)

        # Context vector
        context_vector = attention_weights * hidden_states
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector

    def get_config(self):
        config = super().get_config().copy()
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)

from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM, Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Input Layer
input_layer = Input(shape=(X_train.shape[1], len(features)))

# CNN layers
cnn_output = Conv1D(filters=64, kernel_size=3, activation='elu')(input_layer)
cnn_output = MaxPooling1D(pool_size=2)(cnn_output)
cnn_output = BatchNormalization()(cnn_output)
cnn_output = Dropout(0.3)(cnn_output)
#
# LSTM Layers
# lstm_output = LSTM(128, activation='relu', return_sequences=True)(cnn_output)
# lstm_output = Dropout(0.2)(lstm_output)  # Optional dropout for regularization
lstm_output = LSTM(64, activation='relu', return_sequences=True)(cnn_output)
lstm_output = BatchNormalization()(lstm_output)
# lstm_output = Dropout(0.3)(lstm_output)
# Attention Layer
attention_output = AttentionLayer()(lstm_output)
lstm_output = Dropout(0.2)(attention_output)

# lstm_output = Dense(64, activation='relu')(lstm_output)
# lstm_output = Dropout(0.2)(attention_output)
lstm_output = Dense(32, activation='relu')(lstm_output)
lstm_output = Dropout(0.2)(attention_output)

# Dense Layer for the final output (for regression task)
output = Dense(1, activation='linear')(lstm_output)

# Build the model
model = Model(inputs=input_layer, outputs=output)

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mean_squared_error'  # MSE for regression tasks  # Include MAPE for model evaluation
)

# Model Summary
model.summary()

model.fit(X_train, y_train, epochs=150, batch_size=64, validation_split=0.1)

# LSTM features
lstm_feature_extractor = Model(inputs=model.input, outputs=lstm_output)
lstm_features_train = lstm_feature_extractor.predict(X_train)
lstm_features_test = lstm_feature_extractor.predict(X_test)

from sklearn.metrics import mean_squared_error
import xgboost as xgb

# Reshape the LSTM features to 2D
lstm_features_train_flat = lstm_features_train.reshape((lstm_features_train.shape[0], -1))
lstm_features_test_flat = lstm_features_test.reshape((lstm_features_test.shape[0], -1))

# Train XGBoost using only LSTM features
xgb_model = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.001,objective='reg:squarederror',colsample_bylevel=0.8, colsample_bytree=0.8)
xgb_model.fit(lstm_features_train_flat, y_train)

# Make predictions
y_pred = xgb_model.predict(lstm_features_test_flat)

from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error

# Define a safe MAPE calculation function
def safe_mape(true_values, predicted_values):
    mask = true_values != 0  # Ignore true values that are zero
    return np.mean(np.abs((true_values[mask] - predicted_values[mask]) / true_values[mask])) * 100

mape = safe_mape(y_test,y_pred)

print(f"Mean Absolute Percentage Error (MAPE): {mape}")

